{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d31031",
   "metadata": {},
   "source": [
    "## Azure ML - Sample Pipeline for File Dataset Creation/Consumption\n",
    "This notebook demonstrates creation and execution of an Azure ML pipeline designed to create pandas dataframes filled with random data and save these as CSVs to a File Dataset. This File Dataset is subsequently consumed both as a mount and a download in downstream steps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fba5e8",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8767170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData, PipelineEndpoint\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169cd843",
   "metadata": {},
   "source": [
    "### Connect to Azure ML Workspace, Provision Compute Resources, and get References to Datastores\n",
    "Connect to workspace using config associated config file. Get a reference to you pre-existing AML compute cluster or provision a new cluster to facilitate processing. Finally, get references to your default blob datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc8ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to AML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# #Select AML Compute Cluster\n",
    "cpu_cluster_name = 'cpucluster'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=1)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "#Get default datastore\n",
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bfa798",
   "metadata": {},
   "source": [
    " ### Create Run Configuration\n",
    "The `RunConfiguration` defines the environment used across all python steps. You can optionally add additional conda or pip packages to be added to your environment. [More details here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc028d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.docker.use_docker = True\n",
    "run_config.docker.shm_size='10g'\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['pandas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521e88b",
   "metadata": {},
   "source": [
    "### Define Output Datasets\n",
    "Below we define the configuration for the `FileDataset` that will be passed between steps in our pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a6db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file_dataset = OutputFileDatasetConfig(name='sample_file_dataset', destination=(default_ds, 'sample_file_dataset/{run-id}')).register_on_complete(name='sample_file_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326e069f",
   "metadata": {},
   "source": [
    "### Define Pipeline Steps\n",
    "The pipeline below consists of steps to gather and register data from a remote source, a scoring step where the registered model is used to make predictions on loaded, and a data publish step where scored data can be exported to a remote data source. All of the PythonScriptSteps have a corresponding *.py file which is referenced in the step arguments. Also, any PipelineParameters defined above can be passed to and consumed within these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_data_step = PythonScriptStep(\n",
    "    name='Register File Dataset',\n",
    "    script_name='register_file_dataset.py',\n",
    "    arguments=[\n",
    "        '--sample_file_dataset', sample_file_dataset\n",
    "    ],\n",
    "    outputs=[sample_file_dataset],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "consume_data_as_download_step = PythonScriptStep(\n",
    "    name='Consume File Dataset as Download',\n",
    "    script_name='consume_file_dataset_as_download.py',\n",
    "    arguments=[\n",
    "        '--local_download_dir', './tmpdir'\n",
    "    ],\n",
    "    inputs=[sample_file_dataset.as_input(name='sample_file_dataset').as_download('./tmpdir')],\n",
    "    outputs=[],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "consume_data_as_mount_step = PythonScriptStep(\n",
    "    name='Consume File Dataset as Mount',\n",
    "    script_name='consume_file_dataset_as_mount.py',\n",
    "    arguments=[],\n",
    "    inputs=[sample_file_dataset.as_input(name='sample_file_dataset').as_mount()],\n",
    "    outputs=[],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b935994e",
   "metadata": {},
   "source": [
    "### Create Pipeline\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. Note: based on the dataset dependencies between steps, exection occurs logically such that no step will execute unless all of the necessary input datasets have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d096f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[register_data_step, consume_data_as_download_step, consume_data_as_mount_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0107f3",
   "metadata": {},
   "source": [
    "### Create Experiment and Run Pipeline\n",
    "Define a new experiment (logical container for pipeline runs) and execute the pipeline. You can modify the values of pipeline parameters here when submitting a new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, 'file_dataset_testing')\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
