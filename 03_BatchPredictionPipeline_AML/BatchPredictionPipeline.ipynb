{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e643df",
   "metadata": {},
   "source": [
    "## Azure ML - Sample Batch Prediction Pipeline\n",
    "This notebook demonstrates creation and execution of an Azure ML pipeline designed to load data from a remote source, to make predictions against that data using a previously registered ML model, and finally save that data to an external sink where it can be consumed by other LOB apps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471e9bf",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339bfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData, PipelineEndpoint\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff5700",
   "metadata": {},
   "source": [
    "### Connect to Azure ML Workspace, Provision Compute Resources, and get References to Datastores\n",
    "Connect to workspace using config associated config file. Get a reference to you pre-existing AML compute cluster or provision a new cluster to facilitate processing. Finally, get references to your default blob datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to AML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "cpu_cluster_name = 'cpucluster'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=1)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "#Get default datastore\n",
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9788fadd",
   "metadata": {},
   "source": [
    " ### Create Run Configuration\n",
    "The `RunConfiguration` defines the environment used across all python steps. You can optionally add additional conda or pip packages to be added to your environment. [More details here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f5443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.docker.use_docker = True\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['requests', 'pandas'])\n",
    "run_config.environment.python.conda_dependencies.add_pip_package('snowflake-connector-python[pandas]')\n",
    "run_config.environment.python.conda_dependencies.add_pip_package('scikit-learn==0.24')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01932ab",
   "metadata": {},
   "source": [
    "### Define Output Datasets\n",
    "Below we define the configuration for datasets that will be passed between steps in our pipeline. Note, in all cases we specify the datastore that should hold the datasets and whether they should be registered following step completion or not. This can optionally be disabled by removing the register_on_complete() call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637c9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencing_dataset = OutputFileDatasetConfig(name='inferencing_dataset', destination=(default_ds, 'inferencing_dataset/{run-id}')).read_delimited_files().register_on_complete(name='inferencing_data')\n",
    "scored_dataset = OutputFileDatasetConfig(name='scored_dataset', destination=(default_ds, 'scored_dataset/{run-id}')).read_delimited_files().register_on_complete(name='scored_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5201051f",
   "metadata": {},
   "source": [
    "### Define Pipeline Parameters\n",
    "PipelineParameter objects serve as variable inputs to an Azure ML pipeline and can be specified at runtime. Below we specify a pipeline parameter object `model_name` which will be used to reference the locally trained model that was uploaded and registered within the Azure ML workspace. Multiple pipeline parameters can be created and used. Included here are multiple sample pipeline parameters (`get_data_param_*`) to highlight how parameters can be passed into and consumed by various pipeline steps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = PipelineParameter(name='model_name', default_value='iris_classification')\n",
    "get_data_param_1 = PipelineParameter(name='get_data_param_1', default_value='value_1')\n",
    "get_data_param_2 = PipelineParameter(name='get_data_param_2', default_value='value_2')\n",
    "get_data_param_3 = PipelineParameter(name='get_data_param_3', default_value='value_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11703fb3",
   "metadata": {},
   "source": [
    "### Define Pipeline Steps\n",
    "The pipeline below consists of steps to gather and register data from a remote source, a scoring step where the registered model is used to make predictions on loaded, and a data publish step where scored data can be exported to a remote data source. All of the PythonScriptSteps have a corresponding *.py file which is referenced in the step arguments. Also, any PipelineParameters defined above can be passed to and consumed within these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33859f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_inferencing_data_step = PythonScriptStep(\n",
    "    name='Get Inferencing Data',\n",
    "    script_name='get_inferencing_data.py',\n",
    "    arguments=[\n",
    "        '--get_data_param_1', get_data_param_1,\n",
    "        '--get_data_param_2', get_data_param_2,\n",
    "        '--get_data_param_3', get_data_param_3,\n",
    "        '--inferencing_dataset', inferencing_dataset\n",
    "    ],\n",
    "    outputs=[inferencing_dataset],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='.',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "score_inferencing_data_step = PythonScriptStep(\n",
    "    name='Score Inferencing Data',\n",
    "    script_name='score_inferencing_data.py',\n",
    "    arguments=[\n",
    "        '--model_name', model_name,\n",
    "        '--scored_dataset', scored_dataset\n",
    "    ],\n",
    "    inputs=[inferencing_dataset.as_input(name='inferencing_data')],\n",
    "    outputs=[scored_dataset],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='.',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "publish_scored_data_step = PythonScriptStep(\n",
    "    name='Publish Scored Data',\n",
    "    script_name='publish_scored_data.py',\n",
    "    inputs=[scored_dataset.as_input(name='scored_data')],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='.',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c41a849",
   "metadata": {},
   "source": [
    "### Create Pipeline\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. Note: based on the dataset dependencies between steps, exection occurs logically such that no step will execute unless all of the necessary input datasets have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e92da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[get_inferencing_data_step, score_inferencing_data_step, publish_scored_data_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688ed9a",
   "metadata": {},
   "source": [
    "### Create Experiment and Run Pipeline\n",
    "Define a new experiment (logical container for pipeline runs) and execute the pipeline. You can modify the values of pipeline parameters here when submitting a new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, 'iris_batch_predictions')\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc52446f",
   "metadata": {},
   "source": [
    "### Publish Pipeline\n",
    "Create a published version of your pipeline that can be triggered via a REST API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name = 'Iris Batch Prediction Pipeline',\n",
    "                                     description = 'Pipeline that generates batch predictions using a locally trained model.',\n",
    "                                     continue_on_step_failure = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
